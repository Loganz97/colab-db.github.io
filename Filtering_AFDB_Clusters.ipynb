{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP5l/FoFFLMZOa1KsfxYiFo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Preface:**\n",
        "This notebook takes a filtered set of uniprot IDs and uses it to filter the AFDB clusters full dataset. In the end this allows for the generation of structural clusters based on the AFDB for exploring structural diversity on a more focused set of enzymes."
      ],
      "metadata": {
        "id": "BMUOSnQF8lQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "7CD39R7XBBYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799d1842-e62d-4959-f496-83a210864595"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this cell to determine the number of CPU cores you've been assigned and use this to adjust the number of processes, pool = multiprocessing.Pool(processes=**your number of cores**). If you have 8 cores put 8 here. Typically on a high ram CPU runtime this will be 8 cores on a AMD EPYC 7B12 and a free cpu with be 2 cores on a Intel(R) Xeon(R). It is highly recommended to purchase ColabPro to run a high ram runtime otherwise this notebook will take an extremely long time to run."
      ],
      "metadata": {
        "id": "qcpOJsAN9rzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "print(\"You have \" + str(multiprocessing.cpu_count()) + \" cores in this runtime.\")"
      ],
      "metadata": {
        "id": "_BTuAEAO_NML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first cell takes an input of the uniprot IDs as a TSV file. To get this go to uniprot and use the advanced filters to generate the datset. Uniprot will by default add multiple factors other than just the uniprot ID, uncheck all of these before downloading the dataset."
      ],
      "metadata": {
        "id": "wiFtPNvI86mX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbJ60pUJ7zU7"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# File paths\n",
        "uniprotkb_file_path = '/content/drive/MyDrive/NREL/uniprotkb_go_0016787_AND_length_200_TO_2024_06_03.tsv'\n",
        "afdb_clusters_file_path = '/content/drive/MyDrive/NREL/5-allmembers-repId-entryId-cluFlag-taxId.tsv.gz'\n",
        "\n",
        "#Generates output file with datetime to prevent overwriting previous runs\n",
        "def generate_unique_filename(base_path, extension):\n",
        "    # Get current date and time\n",
        "    now = datetime.now()\n",
        "\n",
        "    # Format as a string in the format 'YYYYMMDD_HHMMSS'\n",
        "    timestamp_str = now.strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    unique_filename = f\"{base_path}_{timestamp_str}.{extension}\"\n",
        "    return unique_filename\n",
        "output_file_path = generate_unique_filename('/content/drive/MyDrive/NREL/FilteredAFDB', 'tsv')\n",
        "\n",
        "# Create the file at output_file_path\n",
        "with open(output_file_path, 'w') as f:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Load the smaller dataset (uniprotkb) into a pandas DataFrame\n",
        "    uniprotkb_df = pd.read_csv(uniprotkb_file_path, sep='\\t', header=None)\n",
        "    uniprotkb_set = set(uniprotkb_df[0])\n",
        "\n",
        "    # Function to filter the large AFDB clusters dataset using the uniprotkb set\n",
        "    def filter_large_file(chunk, uniprotkb_set, output_file_path):\n",
        "        # Filter out rows where the 4th column is missing\n",
        "        chunk = chunk[pd.notna(chunk[3])]\n",
        "\n",
        "        filtered_chunk = chunk[chunk[1].isin(uniprotkb_set)]  # Change here: use column 1 instead of 0\n",
        "\n",
        "        # Write the filtered chunk to the output file\n",
        "        with open(output_file_path, 'a') as f:\n",
        "            filtered_chunk.to_csv(f, sep='\\t', index=False, header=False)\n",
        "\n",
        "        print(\"Processed a chunk\")\n",
        "\n",
        "    # Run the filtering function in parallel\n",
        "    def run_parallel():\n",
        "        # Create a multiprocessing pool with x processes\n",
        "        pool = multiprocessing.Pool(processes=8)\n",
        "\n",
        "        # Read the larger dataset in chunks and filter in parallel, adjust chuck size as needed.\n",
        "        with gzip.open(afdb_clusters_file_path, 'rt') as file:\n",
        "            for chunk in pd.read_csv(file, sep='\\t', header=None, chunksize=10000000):\n",
        "                # Apply the filter_large_file function to each chunk in parallel\n",
        "                pool.apply_async(filter_large_file, args=(chunk, uniprotkb_set, output_file_path))\n",
        "\n",
        "        # Close the pool and wait for all processes to finish\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "    # Function to check and correct the output file\n",
        "    def check_and_correct_output_file(output_file_path, afdb_clusters_file_path):\n",
        "        # Read the output file into a DataFrame\n",
        "        output_df = pd.read_csv(output_file_path, sep='\\t', header=None)\n",
        "\n",
        "        # Find rows with less than 4 columns\n",
        "        incomplete_rows = output_df[output_df.count(axis=1) < 4]\n",
        "\n",
        "        if not incomplete_rows.empty:\n",
        "            # Read the afdb_clusters file into a DataFrame\n",
        "            afdb_clusters_df = pd.read_csv(afdb_clusters_file_path, sep='\\t', header=None)\n",
        "\n",
        "            # For each incomplete row, find the corresponding row in the afdb_clusters file and replace the incomplete row\n",
        "            for index, row in incomplete_rows.iterrows():\n",
        "                identifier = row[1]\n",
        "                correct_row = afdb_clusters_df[afdb_clusters_df[1] == identifier]\n",
        "                output_df.loc[index] = correct_row.values[0]\n",
        "\n",
        "            # Write the corrected DataFrame to the output file\n",
        "            output_df.to_csv(output_file_path, sep='\\t', index=False, header=False)\n",
        "\n",
        "        print(\"Checked and corrected the output file\")\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        multiprocessing.freeze_support()\n",
        "\n",
        "        # Measure the execution time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run the filtering function in parallel\n",
        "        run_parallel()\n",
        "\n",
        "        # Check and correct the output file\n",
        "        check_and_correct_output_file(output_file_path, afdb_clusters_file_path)\n",
        "\n",
        "        # Calculate the execution time\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Execution time: {execution_time} seconds\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "Ejt39XnDF4bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following function removes all singletons, if you want singletons to be included do not run this cell.**"
      ],
      "metadata": {
        "id": "9efXuLtUQda0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing singletons\n",
        "def remove_lines_with_4_in_third_column(filename):\n",
        "    # Read the file into a pandas DataFrame\n",
        "    df = pd.read_csv(filename, sep='\\t', header=None)\n",
        "\n",
        "    total_rows = len(df)\n",
        "    checkpoints = [0.25 * total_rows, 0.5 * total_rows, 0.75 * total_rows]\n",
        "\n",
        "    # Filter out rows where the third column contains a 4\n",
        "    for i, row in df.iterrows():\n",
        "        if row[2] == 4:\n",
        "            df.drop(i, inplace=True)\n",
        "\n",
        "        # Print a message when 25%, 50%, and 75% of the rows are processed\n",
        "        if i in checkpoints:\n",
        "            print(f\"Processed {int((i / total_rows) * 100)}% of the rows\")\n",
        "\n",
        "    # Write the filtered DataFrame back to the file\n",
        "    df.to_csv(filename, sep='\\t', index=False, header=False)\n",
        "\n",
        "# Call the function on the output file\n",
        "remove_lines_with_4_in_third_column(output_file_path)"
      ],
      "metadata": {
        "id": "p60ufXxUP1mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to issues, these file paths have to be updated manually."
      ],
      "metadata": {
        "id": "LjARZzmDrYWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import time\n",
        "from datetime import datetime\n",
        "all_vs_all_clusters_path = '/content/drive/MyDrive/NREL/6-all-vs-all-similarity-queryId_targetId_eValue.tsv.gz'\n",
        "filtered_AFDB_path = '/content/drive/MyDrive/NREL/FilteredAFDB_20240603_234046.tsv'\n",
        "allvall_output_path = '/content/drive/MyDrive/NREL/Filtered_allvsall_alkaline.tsv'\n"
      ],
      "metadata": {
        "id": "xAdILYDwvtfE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running ensure the number of processes is correct. This is expected to take ~35 min when using 8 processors. There are ~1.1 billion lines in this dataset."
      ],
      "metadata": {
        "id": "knZaefYkRSzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Load the smaller dataset into a pandas DataFrame\n",
        "    filtered_AFDB_df = pd.read_csv(filtered_AFDB_path, sep='\\t', header=None)\n",
        "    filtered_AFDB_set = set(filtered_AFDB_df[1])  # Use column 1 for filtering\n",
        "\n",
        "    # Function to filter the large AFDB clusters dataset using the filtered_AFDB_set\n",
        "    def filter_large_file(chunk, filtered_AFDB_set):\n",
        "        filtered_chunk = chunk[chunk[1].isin(filtered_AFDB_set)]  # Use column 1 for filtering\n",
        "        return filtered_chunk\n",
        "\n",
        "    # Run the filtering function in parallel\n",
        "    def run_parallel():\n",
        "        # Create a multiprocessing pool with x processes\n",
        "        pool = multiprocessing.Pool(processes=8)\n",
        "\n",
        "        # Read the larger dataset in chunks and filter in parallel\n",
        "        with gzip.open(all_vs_all_clusters_path, 'rt') as file:\n",
        "            for chunk in pd.read_csv(file, sep='\\t', header=None, chunksize=10000000):\n",
        "                # Apply the filter_large_file function to each chunk in parallel\n",
        "                result = pool.apply_async(filter_large_file, args=(chunk, filtered_AFDB_set))\n",
        "\n",
        "                # Write the filtered chunk to the output file in the main process\n",
        "                filtered_chunk = result.get()\n",
        "                with open(allvall_output_path, 'a') as f:\n",
        "                    filtered_chunk.to_csv(f, sep='\\t', index=False, header=False)\n",
        "\n",
        "        # Close the pool and wait for all processes to finish\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        multiprocessing.freeze_support()\n",
        "\n",
        "        # Measure the execution time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run the filtering function in parallel\n",
        "        run_parallel()\n",
        "\n",
        "        # Calculate the execution time\n",
        "        execution_time = time.time() - start_time\n",
        "        print(f\"Execution time: {execution_time} seconds\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "Jh1b07TJHnFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8bd1e7-0a9d-457f-f093-6f6cdd373be1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: 2104.625983953476 seconds\n"
          ]
        }
      ]
    }
  ]
}